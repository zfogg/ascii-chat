name: 'Publish Test Results'
description: 'Validates Criterion XML, publishes test results to GitHub, uploads to Codecov'
inputs:
  test-type:
    description: 'Type of test (unit, integration, performance)'
    required: true
  os-name:
    description: 'Operating system name (ubuntu, macos)'
    required: true
  build-type:
    description: 'Build type (debug, release, or empty for integration/perf tests)'
    required: false
    default: ''
  criterion-xml-dir:
    description: 'Directory containing Criterion XML files'
    required: false
    default: 'build/Testing/criterion-xml'
  coverage-files:
    description: 'Coverage file pattern for Codecov'
    required: false
    default: './*.gcov'
  codecov-token:
    description: 'Codecov token'
    required: true
  upload-coverage:
    description: 'Whether to upload coverage to Codecov'
    required: false
    default: 'false'

runs:
  using: "composite"
  steps:
    # Validate codecov.yml configuration
    - name: Validate Codecov Configuration
      shell: bash
      run: |
        if [ -f "codecov.yml" ]; then
          echo "Validating codecov.yml configuration..."
          response=$(curl -s --data-binary @codecov.yml https://codecov.io/validate)

          # Check if response starts with "Valid!" (successful validation)
          if echo "$response" | grep -q '^Valid!'; then
            echo "codecov.yml is valid"
            # Extract and display the JSON part (skip first line)
            echo "$response" | tail -n +2 | jq -C '.coverage.status.project | keys' 2>/dev/null || true
          else
            echo "codecov.yml validation failed:"
            echo "$response"
            exit 1
          fi
        else
          echo "No codecov.yml found, skipping validation"
        fi

    # Validate Criterion XML files
    - name: Validate Criterion XML
      if: always()
      shell: bash
      run: |
        xml_dir="${{ inputs.criterion-xml-dir }}"
        if [ -d "$xml_dir" ]; then
          xml_count=$(find "$xml_dir" -name "*.xml" -type f | wc -l)
          echo "Found $xml_count Criterion XML files in $xml_dir"

          if [ "$xml_count" -gt 0 ]; then
            echo "XML files:"
            find "$xml_dir" -name "*.xml" -type f -exec basename {} \;

            # Show sample of first XML file
            first_xml=$(find "$xml_dir" -name "*.xml" -type f | head -1)
            if [ -n "$first_xml" ]; then
              echo "Sample from $(basename $first_xml):"
              head -10 "$first_xml"
            fi

            # Validate with xmllint if available
            if command -v xmllint &>/dev/null; then
              echo "Validating XML files with xmllint..."
              valid_count=0
              invalid_count=0
              for xml_file in "$xml_dir"/*.xml; do
                if [ -f "$xml_file" ]; then
                  if xmllint --noout "$xml_file" 2>/dev/null; then
                    valid_count=$((valid_count + 1))
                  else
                    echo "Invalid XML: $(basename $xml_file)"
                    invalid_count=$((invalid_count + 1))
                  fi
                fi
              done
              echo "Valid: $valid_count, Invalid: $invalid_count"
            fi
          else
            echo "No XML files found in $xml_dir"
          fi
        else
          echo "Criterion XML directory not found: $xml_dir"
          echo "Looking for XML files:"
          find . -name "*.xml" -type f 2>/dev/null | head -10
        fi

    # Publish test results to GitHub checks (Linux)
    - name: Publish Test Results to GitHub (Linux)
      uses: EnricoMi/publish-unit-test-result-action@v2
      if: always() && runner.os == 'Linux'
      with:
        files: |
          ${{ inputs.criterion-xml-dir }}/*.xml
        check_name: |
          ${{ inputs.test-type == 'unit' && format('{0} Tests ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Tests ({1})', 'Integration', inputs.os-name) ||
              format('{0} Tests ({1})', 'Performance', inputs.os-name) }}
        comment_title: |
          ${{ inputs.test-type == 'unit' && format('{0} Test Results ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Test Results ({1})', 'Integration', inputs.os-name) ||
              format('{0} Test Results ({1})', 'Performance', inputs.os-name) }}
        compare_to_earlier_commit: true
        test_changes_limit: 10
        fail_on: 'nothing'  # Don't fail the workflow based on test results

    # Publish test results to GitHub checks (macOS)
    - name: Publish Test Results to GitHub (macOS)
      uses: EnricoMi/publish-unit-test-result-action/macos@v2
      if: always() && runner.os == 'macOS'
      with:
        files: |
          ${{ inputs.criterion-xml-dir }}/*.xml
        check_name: |
          ${{ inputs.test-type == 'unit' && format('{0} Tests ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Tests ({1})', 'Integration', inputs.os-name) ||
              format('{0} Tests ({1})', 'Performance', inputs.os-name) }}
        comment_title: |
          ${{ inputs.test-type == 'unit' && format('{0} Test Results ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Test Results ({1})', 'Integration', inputs.os-name) ||
              format('{0} Test Results ({1})', 'Performance', inputs.os-name) }}
        compare_to_earlier_commit: true
        test_changes_limit: 10
        fail_on: 'nothing'  # Don't fail the workflow based on test results

    # Publish test results to GitHub checks (Windows)
    - name: Publish Test Results to GitHub (Windows)
      uses: EnricoMi/publish-unit-test-result-action/windows@v2
      if: always() && runner.os == 'Windows'
      with:
        files: |
          ${{ inputs.criterion-xml-dir }}/*.xml
        check_name: |
          ${{ inputs.test-type == 'unit' && format('{0} Tests ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Tests ({1})', 'Integration', inputs.os-name) ||
              format('{0} Tests ({1})', 'Performance', inputs.os-name) }}
        comment_title: |
          ${{ inputs.test-type == 'unit' && format('{0} Test Results ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Test Results ({1})', 'Integration', inputs.os-name) ||
              format('{0} Test Results ({1})', 'Performance', inputs.os-name) }}
        compare_to_earlier_commit: true
        test_changes_limit: 10
        fail_on: 'nothing'  # Don't fail the workflow based on test results

    # Upload test results to Codecov
    - name: Upload Test Results to Codecov
      if: always()
      uses: codecov/test-results-action@v1
      with:
        files: ${{ inputs.criterion-xml-dir }}/*.xml
        flags: |
          ${{ inputs.test-type == 'unit' && format('ascii-chat-tests-{0}-{1}', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('ascii-chat-integration-{0}', inputs.os-name) ||
              format('ascii-chat-performance-{0}', inputs.os-name) }}
        token: ${{ inputs.codecov-token }}
        fail_ci_if_error: false

    # Check if coverage files exist before uploading
    - name: Check Coverage Files
      id: check-coverage
      if: ${{ always() && inputs.upload-coverage == 'true' }}
      shell: bash
      run: |
        echo "Checking for coverage files matching: ${{ inputs.coverage-files }}"
        gcov_count=$(ls -1 *.gcov 2>/dev/null | wc -l || echo 0)
        if [ "$gcov_count" -gt 0 ]; then
          echo "Found $gcov_count coverage files"
          echo "has_coverage=true" >> $GITHUB_OUTPUT
        else
          echo "No coverage files found matching pattern: ${{ inputs.coverage-files }}"
          echo "has_coverage=false" >> $GITHUB_OUTPUT
          echo "This may be expected for non-coverage builds (e.g., regular debug vs debug-coverage)"
        fi

    # Upload coverage to Codecov (optional)
    - name: Upload Coverage to Codecov
      if: ${{ always() && inputs.upload-coverage == 'true' && steps.check-coverage.outputs.has_coverage == 'true' }}
      uses: codecov/codecov-action@v4
      with:
        token: ${{ inputs.codecov-token }}
        files: ${{ inputs.coverage-files }}
        directory: .
        flags: |
          ${{ inputs.test-type == 'unit' && format('ascii-chat-tests-{0}-{1}', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('ascii-chat-integration-{0}', inputs.os-name) ||
              format('ascii-chat-performance-{0}', inputs.os-name) }}
        name: |
          ${{ inputs.test-type == 'unit' && format('ascii-chat-coverage-{0}-{1}', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('ascii-chat-integration-coverage-{0}', inputs.os-name) ||
              format('ascii-chat-performance-coverage-{0}', inputs.os-name) }}
        fail_ci_if_error: false
        verbose: true
