name: 'Publish Test Results'
description: 'Validates JUnit XML, publishes test results to GitHub, uploads to Codecov'
inputs:
  test-type:
    description: 'Type of test (unit, integration, performance)'
    required: true
  os-name:
    description: 'Operating system name (ubuntu, macos)'
    required: true
  build-type:
    description: 'Build type (debug, release, or empty for integration/perf tests)'
    required: false
    default: ''
  junit-file:
    description: 'Path to JUnit XML file'
    required: false
    default: 'junit.xml'
  coverage-files:
    description: 'Coverage file pattern for Codecov'
    required: false
    default: './*.gcov'
  codecov-token:
    description: 'Codecov token'
    required: true
  upload-coverage:
    description: 'Whether to upload coverage to Codecov'
    required: false
    default: 'false'

runs:
  using: "composite"
  steps:
    # Validate codecov.yml configuration
    - name: Validate Codecov Configuration
      shell: bash
      run: |
        if [ -f "codecov.yml" ]; then
          echo "üîç Validating codecov.yml configuration..."
          response=$(curl -s --data-binary @codecov.yml https://codecov.io/validate)
          
          # Check if response starts with "Valid!" (successful validation)
          if echo "$response" | grep -q '^Valid!'; then
            echo "‚úÖ codecov.yml is valid"
            # Extract and display the JSON part (skip first line)
            echo "$response" | tail -n +2 | jq -C '.coverage.status.project | keys' 2>/dev/null || true
          else
            echo "‚ùå codecov.yml validation failed:"
            echo "$response"
            exit 1
          fi
        else
          echo "‚ö†Ô∏è No codecov.yml found, skipping validation"
        fi

    # Validate JUnit XML exists and is valid
    - name: Validate JUnit XML
      if: always()
      shell: bash
      run: |
        if [ -f "${{ inputs.junit-file }}" ]; then
          echo "‚úÖ JUnit XML exists ($(wc -l < ${{ inputs.junit-file }}) lines)"
          xmllint --format "${{ inputs.junit-file }}" > /dev/null && echo "‚úÖ Valid XML" || echo "‚ùå Invalid XML"
        else
          echo "‚ùå JUnit XML not found at ${{ inputs.junit-file }}"
          echo "Looking for XML files:"
          find . -name "*.xml" -type f | head -5
        fi

    # Publish test results to GitHub checks (Linux)
    - name: Publish Test Results to GitHub (Linux)
      uses: EnricoMi/publish-unit-test-result-action@v2
      if: always() && runner.os == 'Linux'
      with:
        files: |
          ${{ inputs.junit-file }}
        check_name: |
          ${{ inputs.test-type == 'unit' && format('{0} Tests ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) || 
              inputs.test-type == 'integration' && format('{0} Tests ({1})', 'Integration', inputs.os-name) ||
              format('{0} Tests ({1})', 'Performance', inputs.os-name) }}
        comment_title: |
          ${{ inputs.test-type == 'unit' && format('{0} Test Results ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) || 
              inputs.test-type == 'integration' && format('{0} Test Results ({1})', 'Integration', inputs.os-name) ||
              format('{0} Test Results ({1})', 'Performance', inputs.os-name) }}
        compare_to_earlier_commit: true
        test_changes_limit: 10
        fail_on: 'nothing'  # Don't fail the workflow based on test results

    # Publish test results to GitHub checks (macOS)
    - name: Publish Test Results to GitHub (macOS)
      uses: EnricoMi/publish-unit-test-result-action/macos@v2
      if: always() && runner.os == 'macOS'
      with:
        files: |
          ${{ inputs.junit-file }}
        check_name: |
          ${{ inputs.test-type == 'unit' && format('{0} Tests ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) || 
              inputs.test-type == 'integration' && format('{0} Tests ({1})', 'Integration', inputs.os-name) ||
              format('{0} Tests ({1})', 'Performance', inputs.os-name) }}
        comment_title: |
          ${{ inputs.test-type == 'unit' && format('{0} Test Results ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) || 
              inputs.test-type == 'integration' && format('{0} Test Results ({1})', 'Integration', inputs.os-name) ||
              format('{0} Test Results ({1})', 'Performance', inputs.os-name) }}
        compare_to_earlier_commit: true
        test_changes_limit: 10
        fail_on: 'nothing'  # Don't fail the workflow based on test results

    # Publish test results to GitHub checks (Windows)
    - name: Publish Test Results to GitHub (Windows)
      uses: EnricoMi/publish-unit-test-result-action/windows@v2
      if: always() && runner.os == 'Windows'
      with:
        files: |
          ${{ inputs.junit-file }}
        check_name: |
          ${{ inputs.test-type == 'unit' && format('{0} Tests ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) || 
              inputs.test-type == 'integration' && format('{0} Tests ({1})', 'Integration', inputs.os-name) ||
              format('{0} Tests ({1})', 'Performance', inputs.os-name) }}
        comment_title: |
          ${{ inputs.test-type == 'unit' && format('{0} Test Results ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) || 
              inputs.test-type == 'integration' && format('{0} Test Results ({1})', 'Integration', inputs.os-name) ||
              format('{0} Test Results ({1})', 'Performance', inputs.os-name) }}
        compare_to_earlier_commit: true
        test_changes_limit: 10
        fail_on: 'nothing'  # Don't fail the workflow based on test results

    # Upload test results to Codecov
    - name: Upload Test Results to Codecov
      if: ${{ always() && hashFiles(inputs.junit-file) != '' }}
      uses: codecov/test-results-action@v1
      with:
        files: ${{ inputs.junit-file }}
        flags: |
          ${{ inputs.test-type == 'unit' && format('ascii-chat-tests-{0}-{1}', inputs.os-name, inputs.build-type) || 
              inputs.test-type == 'integration' && format('ascii-chat-integration-{0}', inputs.os-name) ||
              format('ascii-chat-performance-{0}', inputs.os-name) }}
        token: ${{ inputs.codecov-token }}
        fail_ci_if_error: false

    # Check if coverage files exist before uploading
    - name: Check Coverage Files
      id: check-coverage
      if: ${{ always() && inputs.upload-coverage == 'true' }}
      shell: bash
      run: |
        echo "Checking for coverage files matching: ${{ inputs.coverage-files }}"
        if ls ${{ inputs.coverage-files }} 2>/dev/null | head -1 > /dev/null; then
          echo "‚úÖ Found coverage files"
          echo "has_coverage=true" >> $GITHUB_OUTPUT
          ls -la ${{ inputs.coverage-files }} | head -10
        else
          echo "‚ö†Ô∏è No coverage files found matching pattern: ${{ inputs.coverage-files }}"
          echo "has_coverage=false" >> $GITHUB_OUTPUT
          echo "This may be expected for non-coverage builds (e.g., regular debug vs debug-coverage)"
        fi

    # Upload coverage to Codecov (optional)
    - name: Upload Coverage to Codecov
      if: ${{ always() && inputs.upload-coverage == 'true' && steps.check-coverage.outputs.has_coverage == 'true' }}
      uses: codecov/codecov-action@v4
      with:
        token: ${{ inputs.codecov-token }}
        files: ${{ inputs.coverage-files }}
        directory: .
        flags: |
          ${{ inputs.test-type == 'unit' && format('ascii-chat-tests-{0}-{1}', inputs.os-name, inputs.build-type) || 
              inputs.test-type == 'integration' && format('ascii-chat-integration-{0}', inputs.os-name) ||
              format('ascii-chat-performance-{0}', inputs.os-name) }}
        name: |
          ${{ inputs.test-type == 'unit' && format('ascii-chat-coverage-{0}-{1}', inputs.os-name, inputs.build-type) || 
              inputs.test-type == 'integration' && format('ascii-chat-integration-coverage-{0}', inputs.os-name) ||
              format('ascii-chat-performance-coverage-{0}', inputs.os-name) }}
        fail_ci_if_error: false
        verbose: true