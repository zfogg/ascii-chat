name: 'Publish Test Results'
description: 'Validates JUnit XML and publishes test results to GitHub'
inputs:
  test-type:
    description: 'Type of test (unit, integration, performance)'
    required: true
  os-name:
    description: 'Operating system name (ubuntu, macos)'
    required: true
  build-type:
    description: 'Build type (debug, release, or empty for integration/perf tests)'
    required: false
    default: ''
  junit-file:
    description: 'Path to JUnit XML file'
    required: false
    default: 'junit.xml'

runs:
  using: "composite"
  steps:

    # Validate and fix JUnit XML if needed
    - name: Validate and Fix JUnit XML
      if: always()
      shell: bash
      run: |
        if [ -f "${{ inputs.junit-file }}" ]; then
          echo "✅ JUnit XML exists ($(wc -l < ${{ inputs.junit-file }}) lines)"

          # Try to validate the XML
          if xmllint --format "${{ inputs.junit-file }}" > /dev/null 2>&1; then
            echo "✅ Valid XML"
          else
            echo "⚠️ Invalid XML detected, attempting to fix..."

            # Check if the file ends with </testsuites>
            if ! tail -1 "${{ inputs.junit-file }}" | grep -q '</testsuites>'; then
              echo "  Adding missing </testsuites> closing tag"
              echo '</testsuites>' >> "${{ inputs.junit-file }}"
            fi

            # Try validation again
            if xmllint --format "${{ inputs.junit-file }}" > /dev/null 2>&1; then
              echo "✅ XML fixed successfully"
            else
              echo "❌ Could not fix XML, creating minimal valid file"
              # Create a minimal valid JUnit XML
              echo '<?xml version="1.0" encoding="UTF-8"?>' > "${{ inputs.junit-file }}.fixed"
              echo '<testsuites name="ASCII-Chat Tests">' >> "${{ inputs.junit-file }}.fixed"
              echo '  <testsuite name="unknown" tests="0" failures="0" errors="1" time="0.0">' >> "${{ inputs.junit-file }}.fixed"
              echo '    <testcase classname="unknown" name="xml_parse_error" time="0.0">' >> "${{ inputs.junit-file }}.fixed"
              echo '      <error message="Original JUnit XML was malformed" type="XMLParseError">Could not parse original JUnit XML file</error>' >> "${{ inputs.junit-file }}.fixed"
              echo '    </testcase>' >> "${{ inputs.junit-file }}.fixed"
              echo '  </testsuite>' >> "${{ inputs.junit-file }}.fixed"
              echo '</testsuites>' >> "${{ inputs.junit-file }}.fixed"
              mv "${{ inputs.junit-file }}.fixed" "${{ inputs.junit-file }}"
            fi
          fi
        else
          echo "❌ JUnit XML not found at ${{ inputs.junit-file }}"
          echo "Looking for XML files:"
          find . -name "*.xml" -type f | head -5

          # Create a minimal file to prevent downstream errors
          echo '<?xml version="1.0" encoding="UTF-8"?>' > "${{ inputs.junit-file }}"
          echo '<testsuites name="ASCII-Chat Tests">' >> "${{ inputs.junit-file }}"
          echo '  <testsuite name="unknown" tests="0" failures="0" errors="1" time="0.0">' >> "${{ inputs.junit-file }}"
          echo '    <testcase classname="unknown" name="no_results" time="0.0">' >> "${{ inputs.junit-file }}"
          echo '      <error message="No test results found" type="MissingResults">JUnit XML file was not generated</error>' >> "${{ inputs.junit-file }}"
          echo '    </testcase>' >> "${{ inputs.junit-file }}"
          echo '  </testsuite>' >> "${{ inputs.junit-file }}"
          echo '</testsuites>' >> "${{ inputs.junit-file }}"
        fi

    # Publish test results to GitHub checks (Linux)
    - name: Publish Test Results to GitHub (Linux)
      uses: EnricoMi/publish-unit-test-result-action@v2
      if: always() && runner.os == 'Linux'
      with:
        files: |
          ${{ inputs.junit-file }}
        check_name: |
          ${{ inputs.test-type == 'unit' && format('{0} Tests ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Tests ({1})', 'Integration', inputs.os-name) ||
              format('{0} Tests ({1})', 'Performance', inputs.os-name) }}
        comment_title: |
          ${{ inputs.test-type == 'unit' && format('{0} Test Results ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Test Results ({1})', 'Integration', inputs.os-name) ||
              format('{0} Test Results ({1})', 'Performance', inputs.os-name) }}
        compare_to_earlier_commit: true
        test_changes_limit: 10
        fail_on: 'nothing'  # Don't fail the workflow based on test results

    # Publish test results to GitHub checks (macOS)
    - name: Publish Test Results to GitHub (macOS)
      uses: EnricoMi/publish-unit-test-result-action/macos@v2
      if: always() && runner.os == 'macOS'
      with:
        files: |
          ${{ inputs.junit-file }}
        check_name: |
          ${{ inputs.test-type == 'unit' && format('{0} Tests ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Tests ({1})', 'Integration', inputs.os-name) ||
              format('{0} Tests ({1})', 'Performance', inputs.os-name) }}
        comment_title: |
          ${{ inputs.test-type == 'unit' && format('{0} Test Results ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Test Results ({1})', 'Integration', inputs.os-name) ||
              format('{0} Test Results ({1})', 'Performance', inputs.os-name) }}
        compare_to_earlier_commit: true
        test_changes_limit: 10
        fail_on: 'nothing'  # Don't fail the workflow based on test results

    # Publish test results to GitHub checks (Windows)
    - name: Publish Test Results to GitHub (Windows)
      uses: EnricoMi/publish-unit-test-result-action/windows@v2
      if: always() && runner.os == 'Windows'
      with:
        files: |
          ${{ inputs.junit-file }}
        check_name: |
          ${{ inputs.test-type == 'unit' && format('{0} Tests ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Tests ({1})', 'Integration', inputs.os-name) ||
              format('{0} Tests ({1})', 'Performance', inputs.os-name) }}
        comment_title: |
          ${{ inputs.test-type == 'unit' && format('{0} Test Results ({1}-{2})', 'Unit', inputs.os-name, inputs.build-type) ||
              inputs.test-type == 'integration' && format('{0} Test Results ({1})', 'Integration', inputs.os-name) ||
              format('{0} Test Results ({1})', 'Performance', inputs.os-name) }}
        compare_to_earlier_commit: true
        test_changes_limit: 10
        fail_on: 'nothing'  # Don't fail the workflow based on test results


    # Note: Coverage upload is now handled by the generate-coverage action
